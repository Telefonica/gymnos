#
#
#   Trainer
#
#

import os
import GPUtil
import cpuinfo
import platform
import numpy as np

from datetime import datetime
from sklearn.model_selection import train_test_split

from . import trackers
from .logger import get_logger
from .utils.path import chdir
from .utils.io_utils import save_to_json
from .utils.iterator_utils import count
from .utils.timing import elapsed_time

TRAINING_MODEL_DIRECTORY = "model"
TRAININGS_FOLDERNAME = "trainings"
TRAINING_METRICS_FILENAME = "metrics.json"
TRAINING_CALLBACKS_FOLDERNAME = "callbacks"
TRAINING_TRACKINGS_FOLDERNAME = "trackings"
TRAINING_EXECUTIONS_FOLDERNAME = "executions"
TRAINING_EXECUTION_ID_STRFTIME = "%H-%M-%S__%d-%m-%Y"


class Trainer:
    """
    Entrypoint to run experiment given an experiment, a model, a dataset, a training and optionally a
    session and a tracking.
    The run method will create a directory with the following structure:
    TRAININGS_FOLDERNAME/
    └── dataset.name/
        ├── TRAINING_EXECUTIONS_FOLDERNAME/
        │   └── TRAINING_EXECUTION_ID_STRFTIME/  # each execution will be named with the datetime the experiment is run
        │       ├── TRAINING_CALLBACKS_FOLDERNAME/  # artifacts generated by each callback
        │       │   └── training.callbacks[i].name/
        │       ├── TRAINING_METRICS_FILENAME  # filename to keep metrics generated and elapsed times
        │       ├── TRAINING_MODEL_DIRECTORY  # model weights/parameters
        │       │   └── ... # artifacts saved by model
        └── TRAINING_TRACKINGS_FOLDERNAME/  # artifacts generated by each tracker
            └── tracking.trackers[i].name/
    """

    def __init__(self, experiment, model, dataset, training, tracking):
        self.experiment = experiment
        self.model = model
        self.dataset = dataset
        self.training = training
        self.tracking = tracking

        self.logger = get_logger(prefix=self)

    def run(self):
        execution_steps_elapsed = {}
        execution_id = datetime.now().strftime(TRAINING_EXECUTION_ID_STRFTIME)

        self.logger.info("Running experiment: {} ...".format(execution_id))

        # CREATE DIRECTORIES TO STORE TRAININGS EXECUTIONS

        trainings_dataset_path = os.path.join(TRAININGS_FOLDERNAME, self.dataset.name)
        trainings_dataset_trackings_path = os.path.join(trainings_dataset_path, TRAINING_TRACKINGS_FOLDERNAME)
        trainings_dataset_execution_path = os.path.join(trainings_dataset_path, TRAINING_EXECUTIONS_FOLDERNAME,
                                                        execution_id)
        os.makedirs(trainings_dataset_execution_path, exist_ok=True)
        self.logger.info("Creating directory to save training results ({})".format(trainings_dataset_execution_path))

        # RETRIEVE PLATFORM DETAILS

        cpu_info = cpuinfo.get_cpu_info()

        gpus_info = []
        for gpu in GPUtil.getGPUs():
            gpus_info.append({
                "name": gpu.name,
                "memory": gpu.memoryTotal
            })

        platform_details = {
            "python_version": platform.python_version(),
            "python_compiler": platform.python_compiler(),
            "platform": platform.platform(),
            "system": platform.system(),
            "node": platform.node(),
            "architecture": platform.architecture()[0],
            "processor": platform.processor(),
            "cpu": {
                "brand": cpu_info["brand"],
                "cores": cpu_info["count"]
            },
            "gpus": gpus_info
        }

        for name, key in zip(("Python version", "Platform"), ("python_version", "platform")):
            self.logger.debug("{}: {}".format(name, platform_details[key]))

        self.logger.debug("Found {} GPUs".format(len(gpus_info)))

        # DEFINE TRACKER TO STORE METRICS AND SAVE THEM TO JSON LATER

        history_tracker = trackers.History()
        self.tracking.trackers.add(history_tracker)

        # START TRACKING

        self.tracking.trackers.start(run_name=execution_id, logdir=trainings_dataset_trackings_path)

        # LOG TRACKING AND MODEL PARAMETERS

        self.tracking.trackers.log_params(self.tracking.params)
        self.tracking.trackers.log_params(self.model.parameters)

        # LOAD DATASET

        self.logger.info("Loading dataset: {} ...".format(self.dataset.name))

        with elapsed_time() as elapsed:
            X, y = self.dataset.dataset.load_data()

        execution_steps_elapsed["load_data"] = elapsed.s
        self.logger.debug("Loading data took {:.2f}s".format(elapsed.s))

        # SPLIT DATASET INTO TRAIN AND TEST

        self.logger.info("Splitting dataset -> Train: {} | Test: {}".format(self.dataset.samples.train,
                                                                            self.dataset.samples.test))
        X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=self.dataset.samples.train,
                                                            test_size=self.dataset.samples.test,
                                                            shuffle=self.dataset.shuffle,
                                                            random_state=self.dataset.seed)
        # APPLY PREPROCESSORS

        self.logger.info("Applying {} preprocessors ({})".format(len(self.dataset.preprocessor_pipeline),
                                                                 str(self.dataset.preprocessor_pipeline)))

        with elapsed_time() as elapsed:
            self.dataset.preprocessor_pipeline.fit(X_train, y_train)

        execution_steps_elapsed["fit_preprocessors"] = elapsed.s
        self.logger.debug("Fitting preprocessors to train data took {:.2f}s".format(elapsed.s))

        with elapsed_time() as elapsed:
            X_train = self.dataset.preprocessor_pipeline.transform(X_train, data_desc="X_train")
            X_test = self.dataset.preprocessor_pipeline.transform(X_test, data_desc="X_test")

        execution_steps_elapsed["transform_preprocessors"] = elapsed.s
        self.logger.debug("Preprocessing data took {:.2f}s".format(elapsed.s))

        # FIT MODEL

        self.logger.info("Fitting model with {} samples ...".format(count(X_train)))

        # Measure time and temporary change directory in case model wants to save some artifact while fitting
        with elapsed_time() as elapsed, chdir(trainings_dataset_execution_path):
            train_metrics = self.model.model.fit(X_train, y_train, **self.training.parameters)

        execution_steps_elapsed["fit_model"] = elapsed.s
        self.logger.debug("Fitting model took {:.2f}s".format(elapsed.s))

        for metric_name, metric_value in train_metrics.items():
            self.logger.info("Results for {}: Min: {:.2f} | Max: {:.2f} | Mean: {:.2f}".format(metric_name,
                                                                                               np.min(metric_value),
                                                                                               np.max(metric_value),
                                                                                               np.mean(metric_value)))
        self.logger.info("Logging train metrics to trackers".format(len(self.tracking.trackers)))
        self.tracking.trackers.log_metrics(train_metrics)

        # EVALUATE MODEL IF TEST SAMPLES EXIST

        self.logger.info("Evaluating model with {} samples".format(count(X_test)))

        with elapsed_time() as elapsed:
            test_metrics = self.model.model.evaluate(X_test, y_test)

        for metric_name, metric_value in test_metrics.items():
            self.logger.info("Results for {}: Min: {:.2f} | Max: {:.2f} | Mean: {:.2f}".format(metric_name,
                                                                                               np.min(metric_value),
                                                                                               np.max(metric_value),
                                                                                               np.mean(metric_value)))
        execution_steps_elapsed["evaluate_model"] = elapsed.s
        self.logger.debug("Evaluating model took {:.2f}s".format(elapsed.s))

        self.logger.info("Logging test metrics to trackers".format(len(self.tracking.trackers)))
        self.tracking.trackers.log_metrics(test_metrics, prefix="test_")

        # SAVE MODEL
        model_dir = os.path.join(trainings_dataset_execution_path, TRAINING_MODEL_DIRECTORY)
        os.makedirs(model_dir)
        self.logger.info("Saving model")
        self.model.model.save(model_dir)

        metrics = dict(
            elapsed=execution_steps_elapsed,
            metrics=history_tracker.metrics,
            platform=platform_details
        )
        save_to_json(os.path.join(trainings_dataset_execution_path, TRAINING_METRICS_FILENAME), metrics)

        self.logger.info("Metrics, platform information and elapsed times saved to {} file".format(
                         TRAINING_METRICS_FILENAME))

        self.tracking.trackers.end()

        return trainings_dataset_execution_path
